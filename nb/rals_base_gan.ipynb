{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Pytorch Rals-C-SAGAN\n* Ra - Relativistic Average;\n* Ls - Least Squares;\n* C - Conditional;\n* SA - Self-Attention;\n* DCGAN - Deep Convolutional Generative Adversarial Network\n\n<br>\nReferences:\n* https://www.kaggle.com/speedwagon/ralsgan-dogs\n* https://www.kaggle.com/cdeotte/dog-breed-cgan\n* https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/cgan/cgan.py\n* https://github.com/voletiv/self-attention-GAN-pytorch/blob/master/sagan_models.py"},{"metadata":{"trusted":true},"cell_type":"code","source":"# loss_calculation = 'hinge'\nloss_calculation = 'rals'\nbatch_size = 32\ncrop_dog = True #犬のアノテーションを使用するかどうか\nnoisy_label = True #ラベルスムージング的な\nR_uni = (0.85, 0.95) #ラベルスムージングするときのrealの範囲\nF_uni = (0.05, 0.15) #ラベルスムージングするときのfakeの範囲\nGcbn = False # generatorにConditionalBatchNorm2dを使うかどうか\nGlrelu = False # generatorにLeakyLeLUを使うかどうか\nflip_p = 0.0 # RandomHorizontalFlipの割合\nn_epochs = 301\n\nuse_pixelnorm = False\ntest_mode = True # commit時はFalseにしてinternetをoffにする\ncolapse_thre = 0.5 # modecolapseのしきい値\n\n\n# optimizerゾーン\n# G_opt = 'adaboundw'\nG_opt = 'adam'\nG_lr = 0.0002\nG_betas = (0.5, 0.999) #ada系のみ\nG_final_lr=0.5 # adaboundのみ\nG_weight_decay=5e-4 # adaboundのみ\nG_eta_min = 0.0003 # コサインアニーリングのパラメタ\n\n# D_opt = 'adaboundw'\nD_opt = 'adam'\n# D_opt = 'SGD'\nD_lr = 0.0002\nD_betas = (0.5, 0.999) #ada系のみ\nD_final_lr=0.1 # adaboundのみ\nD_weight_decay=0 #adaboundのみ\nD_eta_min = 0.0003","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import requests\nimport datetime\nimport io\nimport os\n\nimport matplotlib.pyplot as plt\n\ndef notify(messages, files=None):\n    url = \"https://notify-api.line.me/api/notify\"\n    token = 'bVQ1eoiM1a8NuLwpw6iRRwnvPruRjRpjWMilh0D8E36'\n    headers = {\"Authorization\" : \"Bearer \"+ token}\n    payload = {\"message\" :  messages}\n\n    r = requests.post(url ,headers = headers ,params=payload,files=files)","execution_count":2,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-08-07T06:58:38.294592Z","start_time":"2019-08-07T06:58:32.807904Z"},"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os\nimport PIL\nimport torchvision\nimport torchvision.datasets as dset\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nimport xml.etree.ElementTree as ET\nimport numpy as np\n\nimport imgaug as ia\nimport imgaug.augmenters as iaa\n\nimport matplotlib.pyplot as plt\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.optim as optim\nfrom torch.nn.init import xavier_uniform_\n\n\nimport time\nimport torch\nimport torch.nn as nn\n\nimport torch.nn.parallel\nimport torch.optim as optim\nfrom torch.nn.utils import spectral_norm\nimport torch.utils.data\nimport torchvision\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import save_image\nimport matplotlib.image as mpimg\n\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\n\n\nimport numpy as np\nimport os\nimport gzip, pickle\nimport tensorflow as tf\nfrom scipy import linalg\nimport pathlib\nimport urllib\nimport warnings\nfrom tqdm import tqdm\nfrom PIL import Image\nimport zipfile\n\nfrom tqdm import tqdm_notebook as tqdm\ntorch.backends.cudnn.deterministic = True\n\nkernel_start_time = time.perf_counter()","execution_count":3,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Helper Blocks"},{"metadata":{"ExecuteTime":{"end_time":"2019-08-07T07:21:26.424904Z","start_time":"2019-08-07T07:21:26.386021Z"},"trusted":true},"cell_type":"code","source":"import math\nimport torch\nfrom torch.optim import Optimizer\n\n\nclass AdaBound(Optimizer):\n    \"\"\"Implements AdaBound algorithm.\n    It has been proposed in `Adaptive Gradient Methods with Dynamic Bound of Learning Rate`_.\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): Adam learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        final_lr (float, optional): final (SGD) learning rate (default: 0.1)\n        gamma (float, optional): convergence speed of the bound functions (default: 1e-3)\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsbound (boolean, optional): whether to use the AMSBound variant of this algorithm\n    .. Adaptive Gradient Methods with Dynamic Bound of Learning Rate:\n        https://openreview.net/forum?id=Bkg3g2R9FX\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), final_lr=0.1, gamma=1e-3,\n                 eps=1e-8, weight_decay=0, amsbound=False):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        if not 0.0 <= final_lr:\n            raise ValueError(\"Invalid final learning rate: {}\".format(final_lr))\n        if not 0.0 <= gamma < 1.0:\n            raise ValueError(\"Invalid gamma parameter: {}\".format(gamma))\n        defaults = dict(lr=lr, betas=betas, final_lr=final_lr, gamma=gamma, eps=eps,\n                        weight_decay=weight_decay, amsbound=amsbound)\n        super(AdaBound, self).__init__(params, defaults)\n\n        self.base_lrs = list(map(lambda group: group['lr'], self.param_groups))\n\n    def __setstate__(self, state):\n        super(AdaBound, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('amsbound', False)\n\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group, base_lr in zip(self.param_groups, self.base_lrs):\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        'Adam does not support sparse gradients, please consider SparseAdam instead')\n                amsbound = group['amsbound']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n                    if amsbound:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                if amsbound:\n                    max_exp_avg_sq = state['max_exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n\n                if group['weight_decay'] != 0:\n                    grad = grad.add(group['weight_decay'], p.data)\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                if amsbound:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n\n                bias_correction1 = 1 - beta1 ** state['step']\n                bias_correction2 = 1 - beta2 ** state['step']\n                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n\n                # Applies bounds on actual learning rate\n                # lr_scheduler cannot affect final_lr, this is a workaround to apply lr decay\n                final_lr = group['final_lr'] * group['lr'] / base_lr\n                lower_bound = final_lr * (1 - 1 / (group['gamma'] * state['step'] + 1))\n                upper_bound = final_lr * (1 + 1 / (group['gamma'] * state['step']))\n                step_size = torch.full_like(denom, step_size)\n                step_size.div_(denom).clamp_(lower_bound, upper_bound).mul_(exp_avg)\n\n                p.data.add_(-step_size)\n\n        return loss\n\nclass AdaBoundW(Optimizer):\n    \"\"\"Implements AdaBound algorithm with Decoupled Weight Decay (arxiv.org/abs/1711.05101)\n    It has been proposed in `Adaptive Gradient Methods with Dynamic Bound of Learning Rate`_.\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): Adam learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        final_lr (float, optional): final (SGD) learning rate (default: 0.1)\n        gamma (float, optional): convergence speed of the bound functions (default: 1e-3)\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsbound (boolean, optional): whether to use the AMSBound variant of this algorithm\n    .. Adaptive Gradient Methods with Dynamic Bound of Learning Rate:\n        https://openreview.net/forum?id=Bkg3g2R9FX\n    \"\"\"\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), final_lr=0.1, gamma=1e-3,\n                 eps=1e-8, weight_decay=0, amsbound=False):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        if not 0.0 <= final_lr:\n            raise ValueError(\"Invalid final learning rate: {}\".format(final_lr))\n        if not 0.0 <= gamma < 1.0:\n            raise ValueError(\"Invalid gamma parameter: {}\".format(gamma))\n        defaults = dict(lr=lr, betas=betas, final_lr=final_lr, gamma=gamma, eps=eps,\n                        weight_decay=weight_decay, amsbound=amsbound)\n        super(AdaBoundW, self).__init__(params, defaults)\n\n        self.base_lrs = list(map(lambda group: group['lr'], self.param_groups))\n\n    def __setstate__(self, state):\n        super(AdaBoundW, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault('amsbound', False)\n\n    def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group, base_lr in zip(self.param_groups, self.base_lrs):\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        'Adam does not support sparse gradients, please consider SparseAdam instead')\n                amsbound = group['amsbound']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state['step'] = 0\n                    # Exponential moving average of gradient values\n                    state['exp_avg'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n                    if amsbound:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                if amsbound:\n                    max_exp_avg_sq = state['max_exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                if amsbound:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n\n                bias_correction1 = 1 - beta1 ** state['step']\n                bias_correction2 = 1 - beta2 ** state['step']\n                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n\n                # Applies bounds on actual learning rate\n                # lr_scheduler cannot affect final_lr, this is a workaround to apply lr decay\n                final_lr = group['final_lr'] * group['lr'] / base_lr\n                lower_bound = final_lr * (1 - 1 / (group['gamma'] * state['step'] + 1))\n                upper_bound = final_lr * (1 + 1 / (group['gamma'] * state['step']))\n                step_size = torch.full_like(denom, step_size)\n                step_size.div_(denom).clamp_(lower_bound, upper_bound).mul_(exp_avg)\n\n                if group['weight_decay'] != 0:\n                    decayed_weights = torch.mul(p.data, group['weight_decay'])\n                    p.data.add_(-step_size)\n                    p.data.sub_(decayed_weights)\n                else:\n                    p.data.add_(-step_size)\n\n        return loss","execution_count":4,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# スペクトラルノルム使ったコンボそう\ndef snconv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n    return spectral_norm(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n                                   stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias))\n# スペクトラルノルム使った全結合層\ndef snlinear(in_features, out_features):\n    return spectral_norm(nn.Linear(in_features=in_features, out_features=out_features))\n\n#スペクトラルノルム使ったエンべ層\ndef sn_embedding(num_embeddings, embedding_dim):\n    return spectral_norm(nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim))\n\n#カーネルのアテンションクラス\nclass Self_Attn(nn.Module):\n    \"\"\" Self attention Layer\"\"\"\n    def __init__(self, in_channels):\n        super(Self_Attn, self).__init__()\n        self.in_channels = in_channels\n        self.snconv1x1_theta = snconv2d(in_channels=in_channels, out_channels=in_channels//8, kernel_size=1, stride=1, padding=0)\n        self.snconv1x1_phi = snconv2d(in_channels=in_channels, out_channels=in_channels//8, kernel_size=1, stride=1, padding=0)\n        self.snconv1x1_g = snconv2d(in_channels=in_channels, out_channels=in_channels//2, kernel_size=1, stride=1, padding=0)\n        self.snconv1x1_attn = snconv2d(in_channels=in_channels//2, out_channels=in_channels, kernel_size=1, stride=1, padding=0)\n        self.maxpool = nn.MaxPool2d(2, stride=2, padding=0)\n        self.softmax  = nn.Softmax(dim=-1)\n        self.sigma = nn.Parameter(torch.zeros(1))\n\n    def forward(self, x):\n        _, ch, h, w = x.size()\n        # Theta path\n        theta = self.snconv1x1_theta(x)\n        theta = theta.view(-1, ch//8, h*w)\n        # Phi path\n        phi = self.snconv1x1_phi(x)\n        phi = self.maxpool(phi)\n        phi = phi.view(-1, ch//8, h*w//4)\n        # Attn map\n        attn = torch.bmm(theta.permute(0, 2, 1), phi)\n        attn = self.softmax(attn)\n        # g path\n        g = self.snconv1x1_g(x)\n        g = self.maxpool(g)\n        g = g.view(-1, ch//2, h*w//4)\n        # Attn_g\n        attn_g = torch.bmm(g, attn.permute(0, 2, 1))\n        attn_g = attn_g.view(-1, ch//2, h, w)\n        attn_g = self.snconv1x1_attn(attn_g)\n        # Out\n        out = x + self.sigma * attn_g\n        return out\n\n    \nclass ConditionalBatchNorm2d(nn.Module):\n    def __init__(self, num_features, num_classes):\n        super().__init__()\n        self.num_features = num_features\n        self.bn = nn.BatchNorm2d(num_features)\n        self.embed = nn.Embedding(num_classes, num_features * 2)\n        self.embed.weight.data[:, :num_features].fill_(1.)  # Initialize scale to 1\n        self.embed.weight.data[:, num_features:].zero_()    # Initialize bias at 0\n\n    def forward(self, inputs):\n        x, y = inputs\n        \n        out = self.bn(x)\n        gamma, beta = self.embed(y).chunk(2, 1)\n        out = gamma.view(-1, self.num_features, 1, 1) * out + beta.view(-1, self.num_features, 1, 1)\n        return out","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generator and Discriminator"},{"metadata":{"trusted":true},"cell_type":"code","source":"class UpConvBlock(nn.Module):\n    \"\"\"\n    n_cl クラス数(120),\n    k_s=カーネルサイズ(4), \n    stride=stride(2), \n    padding=padding(0), \n    bias=バイアス入れるかどうか（False),\n    dropout_p=dropout_p(0.0), \n    use_cbn=Conditional Batch Normalization使うかどうか(True)\n    Lrelu=LeakyReLU使うかどうか(True)(FalseはReLU)\n    slope=Lreluのslope(0.05)\n    \"\"\"\n    def __init__(self, n_input, n_output, n_cl, k_s=4, stride=2, padding=0, \n                 bias=False, dropout_p=0.0, use_cbn=True, Lrelu=True, slope=0.05):\n        super(UpConvBlock, self).__init__()\n        self.use_cbn = use_cbn\n        self.dropout_p=dropout_p\n        self.upconv = spectral_norm(nn.ConvTranspose2d(n_input, n_output, kernel_size=k_s, stride=stride, padding=padding, bias=bias))\n        if use_cbn:\n            self.cond_bn = ConditionalBatchNorm2d(n_output, n_cl)\n        else: \n            self.bn = nn.BatchNorm2d(n_output)\n        if Lrelu:\n            self.activ = nn.LeakyReLU(slope, inplace=True)\n        else:\n            self.activ = nn.ReLU(inplace=True)\n        self.dropout = nn.Dropout2d(p=dropout_p)\n        \n    def forward(self, inputs):\n        x0, labels = inputs\n        \n        x = self.upconv(x0)\n        if self.use_cbn: \n            x = self.activ(self.cond_bn((x, labels)))\n        else:            \n            x = self.activ(self.bn(x))\n        if self.dropout_p > 0.0: \n            x = self.dropout(x)\n        return x\n\n\nclass Generator(nn.Module):\n    def __init__(self, nz=128, num_classes=120, channels=3, nfilt=64,use_cbn=True, Lrelu=True):\n        super(Generator, self).__init__()\n        self.nz = nz\n        self.num_classes = num_classes\n        self.channels = channels\n        \n        self.label_emb = nn.Embedding(num_classes, nz)\n        self.upconv1 = UpConvBlock(2*nz, nfilt*8, num_classes, k_s=4, stride=1, padding=0, use_cbn=use_cbn,Lrelu=Lrelu) #4*4\n        self.upconv2 = UpConvBlock(nfilt*8, nfilt*4, num_classes, k_s=4, stride=2, padding=1,use_cbn=use_cbn,Lrelu=Lrelu) #8*8 \n        self.upconv3 = UpConvBlock(nfilt*4, nfilt*2, num_classes, k_s=4, stride=2, padding=1,use_cbn=use_cbn,Lrelu=Lrelu) # 16*16\n        self.upconv4 = UpConvBlock(nfilt*2, nfilt, num_classes, k_s=4, stride=2, padding=1, use_cbn=use_cbn,Lrelu=Lrelu) #32*32\n        self.upconv5 = UpConvBlock(nfilt, 3, num_classes, k_s=4, stride=2, padding=1, use_cbn=use_cbn,Lrelu=Lrelu) #\n        self.out_conv = spectral_norm(nn.Conv2d(3, 3, 3, 1, 1, bias=False))\n        self.out_activ = nn.Tanh()\n        \n    def forward(self, inputs):\n        z, labels = inputs\n        \n        enc = self.label_emb(labels).view((-1, self.nz, 1, 1))\n        enc = F.normalize(enc, p=2, dim=1)\n        x = torch.cat((z, enc), 1)\n        \n        x = self.upconv1((x, labels))\n        x = self.upconv2((x, labels))\n        x = self.upconv3((x, labels))\n        x = self.upconv4((x, labels))\n        x = self.upconv5((x, labels))\n        x = self.out_conv(x)\n        img = self.out_activ(x)              \n        return img\n    \n    \nclass Discriminator(nn.Module):\n    def __init__(self, num_classes=120, channels=3, nfilt=64):\n        super(Discriminator, self).__init__()\n        self.channels = channels\n        self.num_classes = num_classes\n\n        def down_convlayer(n_input, n_output, k_s=4, stride=2, padding=0, dropout_p=0.0):\n            block = [spectral_norm(nn.Conv2d(n_input, n_output, kernel_size=k_s, stride=stride, padding=padding, bias=False)),\n                     nn.BatchNorm2d(n_output),\n                     nn.LeakyReLU(0.2, inplace=True),\n                    ]\n            if dropout_p > 0.0: block.append(nn.Dropout(p=dropout_p))\n            return block\n        \n        self.label_emb = nn.Embedding(num_classes, 64*64)\n        self.model = nn.Sequential(\n            *down_convlayer(self.channels + 1, nfilt, 4, 2, 1),\n            *down_convlayer(nfilt, nfilt*2, 4, 2, 1),\n            *down_convlayer(nfilt*2, nfilt*4, 4, 2, 1),\n            *down_convlayer(nfilt*4, nfilt*8, 4, 2, 1),\n            spectral_norm(nn.Conv2d(nfilt*8, 1, 4, 1, 0, bias=False)),\n        )\n\n    def forward(self, inputs):\n        imgs, labels = inputs\n\n        enc = self.label_emb(labels).view((-1, 1, 64, 64))\n        enc = F.normalize(enc, p=2, dim=1)\n        x = torch.cat((imgs, enc), 1)   # 4 input feature maps(3rgb + 1label)\n        \n        out = self.model(x)\n        return out.view(-1)\n\n    \ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)        \n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constrideant_(m.bias.data, 0)","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data loader"},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"class DataGenerator(Dataset):\n    def __init__(self, directory, transform=None, n_samples=np.inf, crop_dogs=True):\n        self.directory = directory\n        self.transform = transform\n        self.n_samples = n_samples        \n        self.samples, self.labels = self.load_dogs_data(directory, crop_dogs)\n\n    def load_dogs_data(self, directory, crop_dogs):\n        required_transforms = torchvision.transforms.Compose([\n                torchvision.transforms.Resize(64),\n                torchvision.transforms.CenterCrop(64),\n        ])\n\n        imgs = []\n        labels = []\n        paths = []\n        for root, _, fnames in sorted(os.walk(directory)):\n            for fname in sorted(fnames)[:min(self.n_samples, 999999999999999)]:\n                path = os.path.join(root, fname)\n                paths.append(path)\n\n        for path in paths:\n            # Load image\n            try: img = dset.folder.default_loader(path)\n            except: continue\n            \n            # Get bounding boxes\n            annotation_basename = os.path.splitext(os.path.basename(path))[0]\n            annotation_dirname = next(\n                    dirname for dirname in os.listdir('../input/annotation/Annotation/') if\n                    dirname.startswith(annotation_basename.split('_')[0]))\n                \n            if crop_dogs:\n                tree = ET.parse(os.path.join('../input/annotation/Annotation/',\n                                             annotation_dirname, annotation_basename))\n                root = tree.getroot()\n                objects = root.findall('object')\n                for o in objects:\n                    bndbox = o.find('bndbox')\n                    xmin = int(bndbox.find('xmin').text)\n                    ymin = int(bndbox.find('ymin').text)\n                    xmax = int(bndbox.find('xmax').text)\n                    ymax = int(bndbox.find('ymax').text)\n                    object_img = required_transforms(img.crop((xmin, ymin, xmax, ymax)))\n                    imgs.append(object_img)\n                    labels.append(annotation_dirname.split('-')[1].lower())\n\n            else:\n                object_img = required_transforms(img)\n                imgs.append(object_img)\n                labels.append(annotation_dirname.split('-')[1].lower())\n            \n        return imgs, labels\n    \n    \n    def __getitem__(self, index):\n        sample = self.samples[index]\n        label = self.labels[index]\n        \n        if self.transform is not None: \n            sample = self.transform(sample)\n        return np.asarray(sample), label\n\n    \n    def __len__(self):\n        return len(self.samples)","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training Parameters"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"database = '../input/all-dogs/all-dogs/'\ncrop_dogs = crop_dog\nn_samples = np.inf\nBATCH_SIZE = batch_size\n\nepochs = n_epochs\n\nuse_soft_noisy_labels=noisy_label #ラベルスムージングするかどうか\nloss_calc = loss_calculation\n\nnz = 128\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ntransform = transforms.Compose([transforms.RandomHorizontalFlip(p=flip_p),\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrain_data = DataGenerator(database, transform=transform, n_samples=n_samples, crop_dogs=crop_dogs)\n\ndecoded_dog_labels = {i:breed for i, breed in enumerate(sorted(set(train_data.labels)))}\nencoded_dog_labels = {breed:i for i, breed in enumerate(sorted(set(train_data.labels)))}\ntrain_data.labels = [encoded_dog_labels[l] for l in train_data.labels] # encode dog labels in the data generator\n\n\ntrain_loader = torch.utils.data.DataLoader(train_data, shuffle=True,\n                                           batch_size=BATCH_SIZE, num_workers=4)\n\n\nprint(\"Dog breeds loaded:  \", len(encoded_dog_labels))\nprint(\"Data samples loaded:\", len(train_data))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"netG = Generator(nz, num_classes=len(encoded_dog_labels), nfilt=64,use_cbn=Gcbn, Lrelu=Glrelu).to(device)\nnetD = Discriminator(num_classes=len(encoded_dog_labels), nfilt=64).to(device)\nweights_init(netG)\nweights_init(netD)\nprint(\"Generator parameters:    \", sum(p.numel() for p in netG.parameters() if p.requires_grad))\nprint(\"Discriminator parameters:\", sum(p.numel() for p in netD.parameters() if p.requires_grad))\n\nif G_opt == 'adaboundw':\n    optimizerG = AdaBoundW(netG.parameters(), lr=G_lr, betas=G_betas,final_lr=G_final_lr,weight_decay=G_weight_decay)\nelif G_opt == 'adam':\n    optimizerG = optim.Adam(netG.parameters(), lr=G_lr, betas=G_betas)\n    \nif D_opt == 'adaboundw':\n    optimizerD = AdaBoundW(netD.parameters(), lr=D_lr, betas=D_betas,final_lr=D_final_lr,weight_decay=D_weight_decay)\nelif D_opt == 'adam':\n    optimizerD = optim.Adam(netD.parameters(), lr=D_lr, betas=D_betas)\nelif D_opt == 'SGD':\n    optimizerD = optim.SGD(netD.parameters(), lr=D_lr)\n\nlr_schedulerG = torch.optim.lr_scheduler.ExponentialLR(optimizerG, gamma=.99)\nlr_schedulerD = torch.optim.lr_scheduler.ExponentialLR(optimizerD, gamma=.99)\n\n# lr_schedulerG = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizerG, T_0=epochs//20, eta_min=G_eta_min)\n# lr_schedulerD = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizerD, T_0=epochs//20, eta_min=D_eta_min)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def mse(imageA, imageB):\n        err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n        err /= float(imageA.shape[0] * imageA.shape[1])\n        return err\n\ndef show_generated_img(n_images=5, nz=128):\n    sample = []\n    for _ in range(n_images):\n        noise = torch.randn(1, nz, 1, 1, device=device)\n        dog_label = torch.randint(0, len(encoded_dog_labels), (1, ), device=device)\n        gen_image = netG((noise, dog_label)).to(\"cpu\").clone().detach().squeeze(0)\n        gen_image = gen_image.numpy().transpose(1, 2, 0)\n        sample.append(gen_image)\n        \n    figure, axes = plt.subplots(1, len(sample), figsize=(64, 64))\n    for index, axis in enumerate(axes):\n        axis.axis('off')\n        image_array = (sample[index] + 1.) / 2.\n        axis.imshow(image_array)\n    plt.show()\n    \n    return image_array\n\n    \ndef analyse_generated_by_class(n_images=5):\n    good_breeds = []\n    for l in range(len(decoded_dog_labels)):\n        sample = []\n        for _ in range(n_images):\n            noise = torch.randn(1, nz, 1, 1, device=device)\n            dog_label = torch.full((1,) , l, device=device, dtype=torch.long)\n            gen_image = netG((noise, dog_label)).to(\"cpu\").clone().detach().squeeze(0)\n            gen_image = gen_image.numpy().transpose(1, 2, 0)\n            sample.append(gen_image)\n        \n        d = np.round(np.sum([mse(sample[k], sample[k+1]) for k in range(len(sample)-1)])/n_images, 1)\n        \n        if l % 10 == 0:            \n            print(f\"Generated breed({d}): \", decoded_dog_labels[l])\n            figure, axes = plt.subplots(1, len(sample), figsize=(64, 64))\n            for index, axis in enumerate(axes):\n                axis.axis('off')\n                image_array = (sample[index] + 1.) / 2.\n                axis.imshow(image_array)\n            plt.show()\n        if d < colapse_thre:\n            print(f\"colapse_breed({d}): \", decoded_dog_labels[l])\n            continue  # had mode colapse(discard)\n        good_breeds.append(l)           \n        \n    return good_breeds\n\n\ndef create_submit(good_breeds):\n    print(\"Creating submit\")\n    os.makedirs('../output_images', exist_ok=True)\n    im_batch_size = 100\n    n_images = 10000\n    \n    all_dog_labels = np.random.choice(good_breeds, size=n_images, replace=True)\n    for i_batch in range(0, n_images, im_batch_size):\n        dog_labels = torch.from_numpy(all_dog_labels[i_batch: (i_batch+im_batch_size)]).to(device)\n        noise = torch.randn(im_batch_size, nz, 1, 1, device=device)\n        gen_images = netG((noise, dog_labels))\n        gen_images = (gen_images.to(\"cpu\").clone().detach() + 1) / 2\n        for ii, img in enumerate(gen_images):\n            save_image(gen_images[ii, :, :, :], os.path.join('../output_images', f'image_{i_batch + ii:05d}.png'))\n            \n    import shutil\n    shutil.make_archive('images', 'zip', '../output_images')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training loop"},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.backends.cudnn.benchmark = True\n\nd_loss_log = []\ng_loss_log = []\ndout_real_log = []\ndout_fake_log = []\ndout_fake_log2 = []\n\niter_n = len(train_loader) - 1 #最後の余ったバッチは計算されないから-1\n\nfor epoch in range(epochs):\n    \n    epoch_g_loss = 0.0  # epochの損失和\n    epoch_d_loss = 0.0  # epochの損失和\n    epoch_dout_real = 0.0\n    epoch_dout_fake = 0.0\n    epoch_dout_fake2 = 0.0\n\n    epoch_time = time.perf_counter()\n    if time.perf_counter() - kernel_start_time > 31000:\n            print(\"Time limit reached! Stopping kernel!\"); break\n\n    for ii, (real_images, dog_labels) in enumerate(train_loader):\n        if real_images.shape[0]!= BATCH_SIZE: continue\n        \n        # ラベルにノイズを入れる。そして時々fakeとrealを入れ替える。\n        if use_soft_noisy_labels:\n            real_labels = torch.squeeze(torch.empty((BATCH_SIZE, 1), device=device).uniform_(*R_uni))\n            fake_labels = torch.squeeze(torch.empty((BATCH_SIZE, 1), device=device).uniform_(*F_uni))\n            for p in np.random.choice(BATCH_SIZE, size=np.random.randint((BATCH_SIZE//8)), replace=False):\n                real_labels[p], fake_labels[p] = fake_labels[p], real_labels[p] # swap labels\n        else:\n            real_labels = torch.full((BATCH_SIZE, 1), 1.0, device=device)\n            fake_labels = torch.full((BATCH_SIZE, 1), 0.0, device=device)\n        \n        ############################\n        # (1) Update D network\n        ###########################\n        netD.zero_grad()\n\n        dog_labels = torch.tensor(dog_labels, device=device)\n        real_images = real_images.to(device)\n        noise = torch.randn(BATCH_SIZE, nz, 1, 1, device=device)\n        \n        outputR = netD((real_images, dog_labels))\n        fake_images = netG((noise, dog_labels))\n\n        outputF = netD((fake_images.detach(), dog_labels))\n        if loss_calc == 'rals':\n            errD = (torch.mean((outputR - torch.mean(outputF) - real_labels) ** 2) + torch.mean((outputF - torch.mean(outputR) + real_labels) ** 2))/2\n        elif loss_calc == 'hinge':\n            d_loss_real = torch.nn.ReLU()(1.0 - (outputR - torch.mean(outputF))).mean()\n            d_loss_fake = torch.nn.ReLU()(1.0 + (outputF - torch.mean(outputR))).mean()\n            errD = (d_loss_real + d_loss_fake) / 2\n\n        errD.backward(retain_graph=True)\n        optimizerD.step()\n\n        ############################\n        # (2) Update G network\n        ###########################\n        netG.zero_grad()\n                                          \n        outputF2 = netD((fake_images, dog_labels))\n        if loss_calc == 'rals':\n            errG = (torch.mean((outputR - torch.mean(outputF2) + real_labels) ** 2) + torch.mean((outputF2 - torch.mean(outputR) - real_labels) ** 2)) / 2\n        elif loss_calc == 'hinge':\n            errG = (torch.mean(torch.nn.ReLU()(1.0 + (outputR - torch.mean(outputF2)))) + torch.mean(torch.nn.ReLU()(1.0 - (outputF2 - torch.mean(outputR)))))/2\n            \n        errG.backward()\n        optimizerG.step()\n        \n        lr_schedulerG.step(epoch)\n        lr_schedulerD.step(epoch)\n        \n        # --------------------\n        # 3. 記録\n        # --------------------\n        epoch_d_loss += errD.item()\n        epoch_g_loss += errG.item()\n        epoch_dout_real += outputR.mean().item()\n        epoch_dout_fake += outputF.mean().item()\n        epoch_dout_fake2 += outputF2.mean().item()\n        \n    d_loss_log.append(epoch_d_loss/iter_n)\n    g_loss_log.append(epoch_g_loss/iter_n)\n    dout_real_log.append(epoch_dout_real/iter_n)\n    dout_fake_log.append(epoch_dout_fake/iter_n)\n    dout_fake_log2.append(epoch_dout_fake2/iter_n)\n        \n    if test_mode:\n        print('loss=%s 1epochの中での平均値 \\n %.2fs [%d/%d] Loss_D: %.4f Loss_G: %.4f outputR: %.4f outputF: %.4f / %.4f' % (loss_calc,\n              time.perf_counter()-epoch_time, epoch+1, epochs, d_loss_log[-1], g_loss_log[-1],dout_real_log[-1], dout_fake_log[-1],dout_fake_log2[-1] ))\n        print('  最後のバッチのloss等 \\n    %.2fs [%d/%d] Loss_D: %.4f Loss_G: %.4f outputR: %.4f outputF: %.4f / %.4f' % (\n            time.perf_counter()-epoch_time, epoch+1, epochs, errD.item(), errG.item(),outputR.mean().item(), outputF.mean().item(),outputF2.mean().item() ))\n        if epoch > 3:\n            if (d_loss_log[-2] < 1e-2) and (d_loss_log[-1] < 1e-2):\n                notify(epoch)\n                continue\n                \n    if epoch % 10 == 0:\n        files = []\n        img = show_generated_img(6)\n        plt.imshow(img)\n        buf = io.BytesIO()\n        plt.savefig(buf, format='png')\n        buf.seek(0)\n        files.append({\"imageFile\": buf})\n        message = 'base_gan_adam_ralsの訓練状況を伝えるよ。\\n\\n可愛いワンちゃん出来たかな？ \\n [%d/%d] \\nL_D: %.4f \\nL_G: %.4f \\noutR: %.4f \\noutF: %.4f / %.4f' % (epoch+1, epochs, d_loss_log[-1], g_loss_log[-1],dout_real_log[-1], dout_fake_log[-1],dout_fake_log2[-1] )\n        notify(message, files[0])     ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualise generated results by label and submit"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure()\nplt.title(\"Learning Curve\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"loss\")\n\n# Traing score と Test score をプロット\nplt.plot(d_loss_log,   color=\"r\", label=\"d_loss\")\nplt.plot(g_loss_log,   color=\"g\", label=\"g_loss\")\n\nplt.legend(loc=\"best\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure()\nplt.title(\"Learning Curve\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"loss\")\n\n# Traing score と Test score をプロット\nplt.plot(dout_real_log, color=\"r\", label=\"dout_r\")\nplt.plot(dout_fake_log, color=\"g\", label=\"dout_f\")\n\nplt.legend(loc=\"best\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"nbTranslate":{"displayLangs":["*"],"hotkey":"alt-t","langInMainMenu":true,"sourceLang":"en","targetLang":"fr","useGoogleTranslate":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":1}